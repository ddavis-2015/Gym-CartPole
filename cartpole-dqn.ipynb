{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cartpole-dqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddavis-2015/OpenAI-Gym-Challenges/blob/master/cartpole-dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFB3cVzmdsIt",
        "colab_type": "text"
      },
      "source": [
        "# Install additional software\n",
        "\n",
        "These packages are needed to render animations for the OpenAI Gym environment.\n",
        "\n",
        "Some OpenAI Gym environments will not run without these packages installed even when not rendering animations.\n",
        "\n",
        "Packages must be installed in the order shown to avoid dependancy problems between pyglet and gym."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x3dnX8_dNzp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -qq -y xvfb\n",
        "!pip install gym\n",
        "!pip install pyglet\n",
        "!pip install pyopengl\n",
        "!apt-get install -qq x11-utils \n",
        "!pip install pyvirtualdisplay\n",
        "!pip install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9UXX33uxflf",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q Network model for reinforcement learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITUDqjP-yY9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "EPISODES = 1000\n",
        "MAXSTEPS = 450\n",
        "DENSE_NODES = 6\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=1000)\n",
        "        self.gamma = 0.90    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate (greedy epsilon)\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.980\n",
        "        self.learning_rate = 0.001\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(DENSE_NODES, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(DENSE_NODES, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state, use_epsilon=True):\n",
        "        if use_epsilon and (np.random.rand() <= self.epsilon):\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action index\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma *\n",
        "                          np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=10, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIvI27dnyLWH",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "The task is attempted a certain number of **EPISODES**\n",
        "\n",
        "Each attempt has up to **MAXSTEPS** actions taken\n",
        "\n",
        "The model is saved on each succesful attempt (reaching **MAXSTEPS** actions)\n",
        "\n",
        "A difference from other DQN model examples is that here training only occurs on a failed attempt, using the replay buffer.  This generally uses less training time, whereas training after each action increases training time.\n",
        "\n",
        "Failures are not penalized.  Instead the goal is simply to maximize the reward by keeping the pole upright for as long as possible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZyes-xFWFgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p save\n",
        "\n",
        "def train():\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    done = False\n",
        "    batch_size = 64\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(MAXSTEPS + 1):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            if done or time == MAXSTEPS:\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.3}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                if done and (len(agent.memory) > batch_size):\n",
        "                    agent.replay(batch_size)\n",
        "                break\n",
        "\n",
        "        if time == MAXSTEPS and not done:\n",
        "            print('***saving***')\n",
        "            agent.save(\"./save/cartpole-dqn.h5\")\n",
        "\n",
        "%timeit -n1 -r1 train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAe2Y3jO4K2E",
        "colab_type": "text"
      },
      "source": [
        "# Function to test the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBe-SqzQOoTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(episodes):\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    agent.load(\"./save/cartpole-dqn.h5\")\n",
        "    done = False\n",
        "    success = 0.0\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(MAXSTEPS + 1):\n",
        "            action = agent.act(state, use_epsilon = False)\n",
        "            next_state, _, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            state = next_state\n",
        "            if done or time == MAXSTEPS:\n",
        "                if time == MAXSTEPS:\n",
        "                    success += 1\n",
        "                if (e + 1) % 20 == 0:\n",
        "                    print(\"episode: {}/{}, score: {}, success {:.2f}%\".format(e + 1, EPISODES, time, (success / (e+1)) * 100))\n",
        "                break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4WRXXMZ4Tiq",
        "colab_type": "text"
      },
      "source": [
        "# Report GPU availability and run the test of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoIfycqnTW00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.test.gpu_device_name())\n",
        "print(f\"GPU support: {tf.test.is_built_with_gpu_support()}  CUDA support: {tf.test.is_built_with_cuda()}\")\n",
        "test(EPISODES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hq2SWgNhgWm",
        "colab_type": "text"
      },
      "source": [
        "# Generate an animation for a single test run of the model\n",
        "\n",
        "Rendering time is longer than real-time (much longer) due to JavaScript code generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbUEzWGxgu3O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "dpy = Display(visible=0, size=(400, 300))\n",
        "dpy.start()\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "ANIMATION_SECS = 10 # length of animation in seconds\n",
        "\n",
        "#\n",
        "# 'done' must be a reference object not a value object or this \n",
        "# variable will be garbage collected.  The HTML function and\n",
        "# FuncAnimation object schedule their work with a timer\n",
        "# (instead of a co-routine).\n",
        "#\n",
        "done = [False]\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "frame_rate = env.metadata['video.frames_per_second']\n",
        "print(f\"Frame Rate: {frame_rate}\")\n",
        "frame_count = ANIMATION_SECS * frame_rate\n",
        "interval_ms = 1000 / frame_rate\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "agent.load(\"./save/cartpole-dqn.h5\")\n",
        "\n",
        "state = env.reset()\n",
        "state = np.reshape(state, [1, state_size])\n",
        "\n",
        "img = plt.imshow(env.render(mode = 'rgb_array')) # only call this once\n",
        "plt.axis('off')\n",
        "\n",
        "def dostep(frame, img, done, agent, state, state_size):\n",
        "    if done[0]:\n",
        "        return []\n",
        "\n",
        "    action = agent.act(state, use_epsilon = False)                \n",
        "    next_state, _, done[0], _ = env.step(action)\n",
        "    next_state = np.reshape(next_state, [1, state_size])\n",
        "    np.copyto(state, next_state)\n",
        "\n",
        "    img.set_data(env.render(mode = 'rgb_array')) # just update the data\n",
        "\n",
        "    return [img]\n",
        "\n",
        "ani = animation.FuncAnimation(plt.gcf(),\n",
        "                              dostep,\n",
        "                              frames = frame_count,\n",
        "                              fargs = (img, done, agent, state, state_size),\n",
        "                              interval = interval_ms,\n",
        "                              blit = True)\n",
        "\n",
        "HTML(ani.to_jshtml())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}